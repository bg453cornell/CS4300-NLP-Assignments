{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18bbeeb908fb22d9fbbbffe6c39ae4f7",
     "grade": false,
     "grade_id": "cell-e7c4ba72adf31577",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 1: Keeping Up With Social Information (Part 1)\n",
    "## Â© Cristian Danescu-Niculescu-Mizil 2018\n",
    "## CS/INFO 4300 Language and Information\n",
    "## Due by midnight on Wednesday February 7th\n",
    "\n",
    "This assignment is **individual**.\n",
    "\n",
    "In this assignment we are practicing with post-processing on a conversational dataset taken from the reality TV show \"Keeping Up With The Kardashians\" and gathering some basic statistics about it. \n",
    "\n",
    "In the next assignment (Assignment 2) we will extend these tools to analyze conversational behavior.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "This project aims to help you get comfortable working with the following tools / technologies / concepts:\n",
    "\n",
    "* word tokenization\n",
    "* histogram plotting using `matplotlib`\n",
    "* character analysis via conversational language\n",
    "\n",
    "**Academic Integrity and Collaboration**\n",
    "\n",
    "Note that these projects should be completed individually. As a result, all University-standard academic integrity guidelines must be followed.\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "\n",
    "You are unable to modify the read-only cells.\n",
    "\n",
    "You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented to ensure that your logic is correct.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "You are expected to submit this .ipynb as your submission for Assignment 1. \n",
    "\n",
    "In addition please submit an html copy of the notebook (You can create this by clicking File > Download as > HTML (.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98ee9fc77461651736926701482b1c83",
     "grade": false,
     "grade_id": "cell-97b1c5e4f7df612c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import bs4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f43944ca7e2ba29b18c82a019e743fea",
     "grade": false,
     "grade_id": "cell-7114417ecfb57e62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure that your kernel is using Python3\n",
    "assert sys.version_info.major == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "035ea67b59a29b27e265ca7f315e693e",
     "grade": false,
     "grade_id": "cell-1ced8dfe7b8cc493",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preliminary Data Cleansing\n",
    "**Note: The following content is for you to review to understand how we cleaned and prepared the data for the analysis below.**\n",
    "\n",
    "We will be continuing where we left off from Assignment 0. \n",
    "\n",
    "### Removing duplicates\n",
    "If you are to examine the original transcripts you will see that many of them are near-duplicates, but most are not *perfect* duplicates. This is problematic, because we cannot simply remove identical transcripts.  Furthermore, we cannot just throw away documents that have large overlap, because we would be throwing away the valuable data that is *not* overlapping.\n",
    "\n",
    "We therefore have to treat the transcripts as sequences, rather than as entire documents, and just remove subsequences that overlap.\n",
    "\n",
    "We therefore used a standard python `difflib` package to write the `find_overlaps` function:\n",
    "\n",
    "```python\n",
    "import difflib\n",
    "def find_overlaps(transcript_a, transcript_b, threshold=5):\n",
    "    \"\"\"Find and return the indices of overlapping subsequences between the two transcripts.\n",
    "    Only return overlapping sequences that consist of at least `threshold` entries.\"\"\"\n",
    "    \n",
    "    # We consider that two transcripts overlap when the messages\n",
    "    # and the speakers are the same, but not the timestamp.\n",
    "    \n",
    "    # Massage the transcripts to disregard timestamp information.\n",
    "    # note that a tuple is hashable, so is okay to use for difflib's SequenceMatcher class.\n",
    "    msgs_a = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_a]\n",
    "    msgs_b = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_b]\n",
    "    matcher = difflib.SequenceMatcher(None, msgs_a, msgs_b)\n",
    "    return list(filter(lambda tup: tup[2] >= threshold, matcher.get_matching_blocks()))\n",
    "```\n",
    "\n",
    "We now use the function above to remove duplicate subsequences. At each step, assume we have a list of \"good\" transcripts that have already been processed. When considering a new transcript, we first remove all subsequences that overlap with any of the already processed ones. Then, we split up the chunks that are not removed, and consider each of them a new transcript.\n",
    "\n",
    "```python\n",
    "deduped_transcripts = []\n",
    "all_keys = sorted(transcripts.keys())\n",
    "\n",
    "for key in all_keys:\n",
    "    transcript = transcripts[key]\n",
    "    for _, good_transcript in deduped_transcripts:\n",
    "        overlaps = find_overlaps(transcript, good_transcript)\n",
    "        for idx_a, _, size in overlaps:\n",
    "            transcript[idx_a:idx_a + size] = [None] * size\n",
    "    \n",
    "    for is_not_none, group in groupby(transcript, lambda x: x is not None):\n",
    "        if is_not_none:\n",
    "            subtranscript = list(group)\n",
    "            deduped_transcripts.append((key, subtranscript))\n",
    "```\n",
    "\n",
    "The `deduped_transcripts` are what you are now analyzing for the rest of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31b30dc1f1a7ca25f188c55e99150466",
     "grade": false,
     "grade_id": "cell-e31168ecff8580a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## We are loading the pickle file that contains all the deduped transcripts from Assignment 0\n",
    "with open('deduped_transcripts.pickle','rb') as f:\n",
    "    deduped_transcripts = pickle.load(f)\n",
    "## We are also loading a pickle file of the titles file that we determined in the beginning of Assignment 0\n",
    "with open('titles.pickle','rb') as f:\n",
    "    titles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ac340bfe184ae0ee7ffad51b95850a6",
     "grade": false,
     "grade_id": "cell-6fcb040d322bffc2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Language analysis\n",
    "\n",
    "## Identifying the words\n",
    "It's time to get down to the bread-and-butter of language analysis: the words used.  For simplification, **we consider a word to be a sequence of alphabetical characters. Treat all other characters as delimiters and do not return them.**\n",
    "\n",
    "\n",
    "## Question 1 (Code Completion): Tokenization \n",
    "\n",
    "In the cell below: *Write a function to 'tokenize' a string into the constituent words*. \n",
    "\n",
    "You **must** use regex to satisfy the function specification. We recommend you leverage `re.findall`. \n",
    "\n",
    "Hint: Check out this online regex calculator: [here](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c275969da290fd49dd1b0b1b73f1e253",
     "grade": false,
     "grade_id": "tokenize",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: Array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [x.lower() for x in re.findall(r\"([A-Za-z]+)\", text)]\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0afdd80640d8b7baf38bcf628e616c90",
     "grade": true,
     "grade_id": "tokenize_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert tokenize(\"It's time to get down to the bread-and-butter\") == \\\n",
    "    ['it', 's', 'time', 'to', 'get', 'down', 'to', 'the', 'bread', 'and', 'butter']\n",
    "assert tokenize(\"Life, Liberty, & the Pursuit of Happiness\") == \\\n",
    "    ['life', 'liberty', 'the', 'pursuit', 'of', 'happiness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77916091c2a8e9153253797fce5f74cd",
     "grade": false,
     "grade_id": "cell-d753d9a79894c197",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (Code Completion): Tokenization of Entire Transcript\n",
    "\n",
    "In the cell below complete the function: *Leveraging the tokenize method to tokenize an entire transcript*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4f8e2b0939840c3efa1d96c9b6e97598",
     "grade": false,
     "grade_id": "tokenize_transcript",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_transcript(tokenize_method,input_transcript):\n",
    "    \"\"\"Returns a list of words contained in an entire transcript.\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcript: Tuple}\n",
    "    Returns: Array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    words = []\n",
    "    for lines in input_transcript[1]:\n",
    "        words.extend(tokenize_method(lines[\"text\"]))\n",
    "    return words\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92be9453913c58275449924259617f4c",
     "grade": true,
     "grade_id": "tokenize_transcript_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert len(tokenize_transcript(tokenize,deduped_transcripts[0])) > 6000 and \\\n",
    "    len(tokenize_transcript(tokenize,deduped_transcripts[0])) < 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ae069ddad9be31a4dd862d046cd4063f",
     "grade": false,
     "grade_id": "cell-659f2ecbe807eaef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (Code Completion) Number of Tokens\n",
    "In the cell below write a function to *count how many tokens are used in the deduplicated transcripts in total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "06633de591351b3692af3186d813fb79",
     "grade": false,
     "grade_id": "num_dedup_tokens",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_dedup_tokens(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             tokenize_transcript_method: Function (a -> b,c),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    cnt = 0\n",
    "    for transcript in input_transcripts:\n",
    "        cnt += len(tokenize_transcript_method(tokenize_method, transcript))\n",
    "    return cnt\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f0a643cce038f516e70acd0971486d1",
     "grade": true,
     "grade_id": "num_dedup_tokens_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) > 200000 and \\\n",
    "    num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) < 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21444bcf0ce4e6b02466592e96f9b226",
     "grade": false,
     "grade_id": "cell-ed6fa2191c79a632",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4 (Code Completion) Number of Distinct Words\n",
    "\n",
    "In the cell below write a function to *count how many distinct words are in the deduplicated transcripts in total*. \n",
    "\n",
    "Hint: Use a *set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "62bf40391c90ff53e74f688bd3894b01",
     "grade": false,
     "grade_id": "num_distinct_words",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_distinct_words(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of distinct tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             tokenize_transcript_method: Function (a -> b,c),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    uni_words = set()\n",
    "    for transcript in input_transcripts:\n",
    "        uni_words.update(tokenize_transcript_method(tokenize_method, transcript))\n",
    "    return len(uni_words)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf42c86a2de1a7a3356c85a46898ef54",
     "grade": true,
     "grade_id": "num_distict_words_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) > 8000 and \\\n",
    "    num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) < 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a5ebefa3e463c845c65a799653ed924",
     "grade": false,
     "grade_id": "cell-eb36dc2695961bb0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 5 (Code Completion) Word Episode Counts\n",
    "\n",
    "This question is asking you to build a dictionary `word_episode_count[word]` = *number of episodes in which it appears*. \n",
    "\n",
    "*Note: Keep in mind that the de-duplicated transcripts don't have unique titles!*\n",
    "\n",
    "In the cell below write a function that: *for each distinct (unique) word, in how many different episodes does it appear?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "923efbe6de9233244c7f5ee83961f34b",
     "grade": false,
     "grade_id": "build_word_episode_count",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_word_episode_count(tokenize_method,tokenize_transcript_method,input_transcripts,input_titles):\n",
    "    \"\"\"Returns a dictionary with the number of episodes each distinct word appears\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 tokenize_transcript_method: Function (a -> b,c),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_titles: Dictionary}\n",
    "        Returns: Dict\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # BUILD UNIQUE WORD LIST\n",
    "    uni_words = set()\n",
    "    for transcript in input_transcripts:\n",
    "        uni_words.update(tokenize_transcript_method(tokenize_method, transcript))\n",
    "    \n",
    "    # BUILD TOKENIZED TRANSCRIPTS DICTIONARY\n",
    "    dict_tokenized_transcripts = []\n",
    "    for transcript in input_transcripts:\n",
    "            dict_tokenized_transcripts.append \\\n",
    "            ((transcript[0],tokenize_transcript_method(tokenize_method, transcript)))\n",
    "    \n",
    "    # BUILD WORD EPISODE COUNT DICTIONARY\n",
    "    word_episode_count = dict()\n",
    "    for word in uni_words:\n",
    "        appear = set()\n",
    "        for title_num, list_word in dict_tokenized_transcripts:\n",
    "            if word in list_word:\n",
    "                appear.add(input_titles[title_num])\n",
    "        word_episode_count[word] = len(appear)\n",
    "        \n",
    "    return word_episode_count\n",
    "    \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a28826d87ccf56c9866f9974edcc8ef8",
     "grade": false,
     "grade_id": "cell-0a14a82efd4f0e23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_episode_count = build_word_episode_count(tokenize,tokenize_transcript,deduped_transcripts,titles)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d8d868508581c5a61fda120a6465ae9",
     "grade": true,
     "grade_id": "build_word_episode_count_test",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_word_episode_count returns the correct output\"\"\"\n",
    "assert word_episode_count['quarter'] == 2\n",
    "assert word_episode_count['made'] == 40\n",
    "assert word_episode_count['never'] == 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ce08841cfcaf794e3af968e28b4962ca",
     "grade": false,
     "grade_id": "cell-88307b2867a85c7d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 6 (Free Response): Distribution Analysis\n",
    "\n",
    "For this question you will be plotting a histogram of the distribution of the number of episodes in which words appear. \n",
    "\n",
    "The *x axis* should correspond to the *number of episodes* in which a word is mentioned, and the *y axis* should show *how many words* are in each bin. \n",
    "\n",
    "Note: Use the default matplotlib settings.\n",
    "\n",
    "Create a new cell with the histogram below. And give an analysis of the histogram: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "83d43ececa794b83dfd9df9ca0bb125b",
     "grade": true,
     "grade_id": "build_word_episode_count_ans",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f691a10d128>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f691a5ed908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.hist(word_episode_count.values())\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Number of words')\n",
    "plt.legend(['bin width = 5.2'])\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram above we can find that over $7000$ words are mentioned less or equal than $5$ episodes while only a few words appear in over $10$ episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2a64274aabfd6c9a230dbd71e09c26f",
     "grade": false,
     "grade_id": "cell-b78c4d64909ac049",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 7 (Code Completition): Good Types\n",
    "\n",
    "For the following question you will build an alphabetically sorted list of all words that appear in **more than one episode.** We shall refer to these words as *good types*. The strategy here is to eliminate very specific words that occur too rarely. The word *type* here refers to word types. Word types are unique words, i.e. *hello* and *goodbye* are two distinct word types. [\"You say goodbye, and I say hello, hello, hello\"](https://www.youtube.com/watch?v=rblYSKz_VnI&feature=youtu.be&t=16s) (warning -- link will play sound) is a sequence of 9 tokens (ignoring punctuation) and contains one *goodbye* type token, and three *hello* type tokens.\n",
    "\n",
    "In the cell below fufill the specifications above and have the function: *produce an alphabetically sorted list of good types*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4301fea0ff7bfc0a76bd906351742f4",
     "grade": false,
     "grade_id": "output_good_types",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def output_good_types(input_word_counts):\n",
    "    \"\"\"Returns a list of good types in alphabeitcally sorted order\n",
    "        Params: {input_word_counts: Dict}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    word_list = []\n",
    "    for word in input_word_counts.keys():\n",
    "        if input_word_counts[word] > 1:\n",
    "            word_list.append(word)\n",
    "    return sorted(word_list)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0dbf338f4231a604985d91122c842938",
     "grade": false,
     "grade_id": "cell-8f0bb8ce404b6b24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_types = output_good_types(word_episode_count)\n",
    "n_good_types = len(good_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4851259a4e62da9dfae9098374ca365",
     "grade": false,
     "grade_id": "cell-800cf3466c807a90",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 7b (Free Response): Good Types\n",
    "\n",
    "In the cell below answer the following: *How many good_types there are? What are the first 10?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "92ab05da789cb6c60699f2e1842da6c2",
     "grade": true,
     "grade_id": "output_good_types_ans",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good types:  4517\n",
      "['a', 'aah', 'ability', 'able', 'about', 'absolute', 'absolutely', 'absurd', 'accent']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print ('Number of good types: ', n_good_types)\n",
    "print (good_types[0:9])\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05c822ec869f48bd52d36605cf2234a0",
     "grade": true,
     "grade_id": "output_good_types_test",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that output_good_types returns the correct output\"\"\"\n",
    "assert n_good_types > 4500 and n_good_types < 5000\n",
    "assert good_types[0:5] == ['a','aah','ability','able','about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "364c3bdb0f8bacc88b61c4bac10d1022",
     "grade": false,
     "grade_id": "cell-dd25b6c0ee0f7aa7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "From now on, the use of array data structures from `numpy` is highly recommended and required in some places. If you've never used numpy before, it will take some time to get used to. In general, numpy is a library that you can use to handle vector/matrix/tensor operations, including creation, modification, and compositions (add, sub, mul, etc.). For example, try running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d70575ee2520d8663a5626a206cd5f6",
     "grade": false,
     "grade_id": "cell-a35c29d2a336c43f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 6 2]]\n",
      "[[1 2]\n",
      " [5 3]\n",
      " [1 1]]\n",
      "[[14 11]\n",
      " [36 28]]\n",
      "[5 8 5]\n",
      "[3 8 2]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,6,2]]) \n",
    "B = np.array([[1,2],[5,3],[1,1]])\n",
    "print(A) #A is a 2x3 matrix\n",
    "print(B) #B is a 3x2 matrix\n",
    "print(A.dot(B)) #A.dot(B) is a 2 by 2 matrix\n",
    "print(np.sum(A, axis=0)) #np.sum(A, axis=0) sums along columns\n",
    "print(np.sum(B, axis=1)) #np.sum(B, axis=1) sums along rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "609638d96f460c027d4bd14392cfbda2",
     "grade": false,
     "grade_id": "cell-be083b1847639a79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You're welcome to find your own resources to learn more about numpy (there are lots of them) but one good introduction is [Justin Johnson's writeup](http://cs231n.github.io/python-numpy-tutorial/#numpy).\n",
    "\n",
    "We will be using vectors and arrays with *n_good_types* columns, such that each good type corresponds to a column, in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e69f8c7cfb3d74d02450cd6e670d913",
     "grade": false,
     "grade_id": "cell-f473f9073805a3f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 8 (Code Completion): Word Frequencies\n",
    "\n",
    "What can you say about the most frequently words used? Would you expect to find the same ordering of frequent words in, say, the NY Times?\n",
    "\n",
    "Note that we are talking about how many times a word appears over all episodes, rather than how many episodes a word appears in.\n",
    "\n",
    "In the cell bellow, complete the function to *find the most frequently occurring words (\"good types\")*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "10d7946435ec30e1699ce35e47e68f23",
     "grade": false,
     "grade_id": "create_ranked_good_types",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_ranked_good_types(tokenize_method,tokenize_transcript_method,input_transcripts,input_good_types):\n",
    "    \"\"\"Returns a list of good types in reverse sorted order in the form:\n",
    "        [(word_1,word_count_1),\n",
    "        ...\n",
    "        (word_10,word_count_10)]\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 tokenize_transcript_method: Function (a -> b,c),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_good_types: List}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # BUILD TOKENIZED TRANSCRIPTS LIST\n",
    "    dict_tokenized_transcripts = []\n",
    "    for transcript in input_transcripts:\n",
    "        dict_tokenized_transcripts.extend(tokenize_transcript_method(tokenize_method, transcript))\n",
    "    \n",
    "    # BUILD WORD EPISODE COUNT DICTIONARY\n",
    "    good_type_appear_count = []\n",
    "    for good_type in input_good_types:\n",
    "        cnt = 0\n",
    "        for word in dict_tokenized_transcripts:\n",
    "            if word == good_type:\n",
    "                    cnt += 1\n",
    "        good_type_appear_count.append((good_type, cnt))\n",
    "        \n",
    "    return sorted(good_type_appear_count, key = lambda g_type: g_type[1], reverse = True)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f708f2c7d28b8d245fe818b6153fcee",
     "grade": false,
     "grade_id": "cell-8bb7a64aaa1b1620",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ranked_counts = create_ranked_good_types(tokenize,tokenize_transcript,deduped_transcripts,good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e950f27d7e8e54664121a7027b20edea",
     "grade": true,
     "grade_id": "create_ranked_good_types_test",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_ranked_good_types returns the correct output\"\"\"\n",
    "assert ranked_counts[0:2] == [('i',13972),('you',10920)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "03d52d40e46f47d7f71f8b03fdd7ddae",
     "grade": false,
     "grade_id": "cell-823c533eca6e490a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 8b (Free Response): Word Frequencies\n",
    "\n",
    "Use the next cell to output the 10 most frequently occurring words (\"good types\") in the following format:\n",
    "\n",
    "**Answer format:**\n",
    "\n",
    "```\n",
    "word_count_1 word_1\n",
    "...\n",
    "word_count_10 word_10 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3e32cac945ade10566d50ea99a5d1988",
     "grade": true,
     "grade_id": "create_ranked_good_types_ans",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i      13972  \n",
      "you    10920  \n",
      "to     8790   \n",
      "s      6651   \n",
      "the    6178   \n",
      "and    5996   \n",
      "a      5485   \n",
      "it     5133   \n",
      "that   4394   \n",
      "t      3591   \n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for i in ranked_counts[0:10]:\n",
    "    print('%-7s%-7s' % i)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2897f70bebef640bcba17420e965a444",
     "grade": false,
     "grade_id": "cell-3f52d6dd8641a84e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Characterizing characters' language\n",
    "\n",
    "Moving on, we will only be considering a subset of characters, arguably the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddbabcc279eb2d98193eb9181daf839d",
     "grade": false,
     "grade_id": "cell-87f62951498c6a21",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_speakers = [u'BRUCE',\n",
    "                 u'JONATHAN',\n",
    "                 u'KHLOE',\n",
    "                 u'KIM',\n",
    "                 u'KOURTNEY',\n",
    "                 u'KRIS',\n",
    "                 u'ROBERT',\n",
    "                 u'SCOTT']\n",
    "\n",
    "n_speakers = len(good_speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7bd775c05f9a92a6491d4ffb7ab123ce",
     "grade": false,
     "grade_id": "cell-1e73cf04c0942e6e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 9 (Code Completion): Character Word Frequencies\n",
    "\n",
    "In the cell below you will be asked to determine the *most frequently used words by each character*\n",
    "\n",
    "This function will require you to return a numpy array of shape `n_speakers` by `n_good_types` such that the entry `(i,j)` indicates how often speaker i says word j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e3338eefd58c7f93d180d2e5fa95b27",
     "grade": false,
     "grade_id": "create_word_freq_array",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_word_freq_array(\n",
    "    tokenize_method,\n",
    "    input_transcripts,\n",
    "    input_speakers,\n",
    "    input_good_types):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j.\n",
    "    \n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcripts: Tuple,\n",
    "             input_speakers: List,\n",
    "             input_good_types: List}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    word_freq_array = np.zeros((len(input_speakers), len(input_good_types)))\n",
    "    \n",
    "    for transcript in input_transcripts:\n",
    "        for line in transcript[1]:\n",
    "            if line['speaker'] in input_speakers:\n",
    "                i = input_speakers.index(line['speaker'])\n",
    "                tokenized_line = tokenize_method(line['text'])\n",
    "                for word in tokenized_line:\n",
    "                    if word in input_good_types:\n",
    "                        j = input_good_types.index(word)\n",
    "                        word_freq_array[i][j] += 1\n",
    "    return word_freq_array\n",
    "   \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59c236073b8d8cefebf25063dce891bb",
     "grade": false,
     "grade_id": "cell-4229598e63922f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_array = create_word_freq_array(tokenize,deduped_transcripts,good_speakers,good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a2ec55ec0a5d18bce9e4500e1506179",
     "grade": true,
     "grade_id": "create_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(word_array[0]) > 26000.0\n",
    "assert sum(word_array[:,3]) > 40 and sum(word_array[:,3]) < 50\n",
    "assert type(word_array) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16bfe0122236bc75b8fccc937595f2ba",
     "grade": false,
     "grade_id": "cell-55cdd926451a2321",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 9b (Free Response): Character Word Frequencies\n",
    "\n",
    "In the cell below, output the *top 10 most frequent words used by each character* in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "78044ad7e01b7c42169efbf0ef37df2b",
     "grade": true,
     "grade_id": "create_word_freq_array_ans",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "i\n",
      "you\n",
      "the\n",
      "s\n",
      "to\n",
      "it\n",
      "a\n",
      "and\n",
      "that\n",
      "this\n",
      "\n",
      "JONATHAN\n",
      "i\n",
      "you\n",
      "to\n",
      "the\n",
      "s\n",
      "a\n",
      "it\n",
      "and\n",
      "like\n",
      "this\n",
      "\n",
      "KHLOE\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "a\n",
      "the\n",
      "it\n",
      "that\n",
      "like\n",
      "\n",
      "KIM\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "the\n",
      "it\n",
      "a\n",
      "that\n",
      "like\n",
      "\n",
      "KOURTNEY\n",
      "i\n",
      "to\n",
      "you\n",
      "and\n",
      "s\n",
      "it\n",
      "the\n",
      "a\n",
      "that\n",
      "t\n",
      "\n",
      "KRIS\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "and\n",
      "it\n",
      "that\n",
      "t\n",
      "\n",
      "ROBERT\n",
      "i\n",
      "to\n",
      "you\n",
      "s\n",
      "and\n",
      "a\n",
      "the\n",
      "just\n",
      "it\n",
      "that\n",
      "\n",
      "SCOTT\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "it\n",
      "that\n",
      "and\n",
      "t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for i in range(len(good_speakers)):\n",
    "    print (good_speakers[i])\n",
    "    combine_good_type_freq = list(zip(good_types, word_array[i]))\n",
    "    top_ten_words = sorted(combine_good_type_freq, key = lambda word: word[1], reverse = True)[0:10]\n",
    "    for word in top_ten_words:\n",
    "        print (word[0])\n",
    "    print ()\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "09df9cda5d911589269a5ec300259d7b",
     "grade": false,
     "grade_id": "cell-e055c738cea3d3b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 10 (Code Completion): Specific Word Usage by Character\n",
    "\n",
    "The above didn't help much in understanding each character's diction, because common words are used too commonly anyway. We want to give more weight to less frequent words, as they carry more information on the particularities of the characters.\n",
    "\n",
    "A simple way to do this is to score the words according to the ratio between how often a given character said the word and how often any of the *good speakers* said it.  \n",
    "\n",
    "This can be accomplished by dividing each columns in the `word_array` matrix by its sum.\n",
    "\n",
    "Note: as some words might never be said by the key characters we are considering, apply *smoothing* by adding 1 to the sum of each column. Do this for each column, regardless of if smoothing is required for the math to work out.\n",
    "\n",
    "In the cell below, complete the function to return a weighted numpy array of *specific* words used by each character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e70401692211be6c62be8de355049d89",
     "grade": false,
     "grade_id": "create_weighted_word_freq_array",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_weighted_word_freq_array(input_word_array):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j weighted by the above ratio.\n",
    "    \n",
    "    Note: You must apply smoothing (by adding 1 to each column)\n",
    "    \n",
    "    Params: {input_word_array: Numpy Array}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    sum_by_column = 1 + np.sum(input_word_array, axis = 0)\n",
    "    return input_word_array / sum_by_column\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95c9df1b4bb8a4e6ea333e48920f3072",
     "grade": false,
     "grade_id": "cell-f3e0cd3d8309ecea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "weighted_words = create_weighted_word_freq_array(word_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4a443b75be216be377f68a9efd0e8ea",
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(weighted_words[:,7]) > 0.7\n",
    "assert type(weighted_words) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ebf5b5a67c919666ff42cdd44ec11b1",
     "grade": false,
     "grade_id": "cell-b2b74adae027c815",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 10b (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Use the next cell to output the top 10 most *specific* words used by each character in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a97760c30b734888024f66f68e96ed59",
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_ans",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "0.89 genetic \n",
      "0.88 hobby   \n",
      "0.86 planners\n",
      "0.83 brake   \n",
      "0.83 carpool \n",
      "0.83 presentation\n",
      "0.80 airplanes\n",
      "0.80 fooling \n",
      "0.80 language\n",
      "0.80 mcdonald\n",
      "\n",
      "JONATHAN\n",
      "0.91 erika   \n",
      "0.89 katie   \n",
      "0.89 pics    \n",
      "0.80 awareness\n",
      "0.76 simon   \n",
      "0.75 beckham \n",
      "0.75 carmen  \n",
      "0.75 command \n",
      "0.75 homely  \n",
      "0.75 oats    \n",
      "\n",
      "KHLOE\n",
      "0.95 fur     \n",
      "0.93 basic   \n",
      "0.90 apparently\n",
      "0.89 campaign\n",
      "0.89 secure  \n",
      "0.88 begins  \n",
      "0.88 fulfilled\n",
      "0.88 furs    \n",
      "0.88 moral   \n",
      "0.86 creepy  \n",
      "\n",
      "KIM\n",
      "0.91 amusing \n",
      "0.89 frizz   \n",
      "0.89 song    \n",
      "0.86 challenge\n",
      "0.83 hotwire \n",
      "0.83 humiliating\n",
      "0.83 punched \n",
      "0.81 airport \n",
      "0.80 advantage\n",
      "0.80 bruised \n",
      "\n",
      "KOURTNEY\n",
      "0.90 ho      \n",
      "0.88 defensive\n",
      "0.83 busted  \n",
      "0.83 escalated\n",
      "0.80 absurd  \n",
      "0.80 cushion \n",
      "0.80 driveway\n",
      "0.80 prevent \n",
      "0.80 sober   \n",
      "0.80 theb    \n",
      "\n",
      "KRIS\n",
      "0.90 sweetie \n",
      "0.89 cristal \n",
      "0.88 cranky  \n",
      "0.86 angeles \n",
      "0.86 chat    \n",
      "0.86 kenneth \n",
      "0.86 los     \n",
      "0.86 shops   \n",
      "0.83 backup  \n",
      "0.83 blew    \n",
      "\n",
      "ROBERT\n",
      "0.88 email   \n",
      "0.86 anal    \n",
      "0.86 truly   \n",
      "0.83 adrian  \n",
      "0.83 scream  \n",
      "0.80 acceptable\n",
      "0.80 overnight\n",
      "0.76 spoken  \n",
      "0.75 bashing \n",
      "0.75 cheating\n",
      "\n",
      "SCOTT\n",
      "0.86 sitter  \n",
      "0.75 exclamation\n",
      "0.75 gentlemen\n",
      "0.71 hooker  \n",
      "0.67 absolute\n",
      "0.67 amigos  \n",
      "0.67 assaulted\n",
      "0.67 bateman \n",
      "0.67 blessing\n",
      "0.67 brucinator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for i in range(len(good_speakers)):\n",
    "    print (good_speakers[i])\n",
    "    combine_good_type_freq = list(zip(good_types, weighted_words[i]))\n",
    "    top_ten_words = sorted(combine_good_type_freq, key = lambda word: word[1], reverse = True)[0:10]\n",
    "    for word in top_ten_words:\n",
    "        print ('{0: <3.2f} {1:8s}'.format(word[1], word[0]))\n",
    "    print ()\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "caa10a6d33de0894f33cde7e424e2f89",
     "grade": false,
     "grade_id": "cell-8c3a58ca841fda7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 10c (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Now we can start to see interesting differences between the characters.\n",
    "\n",
    "Create a new Markdown cell bellow and use it to write a paragraph discussing the differences you find most striking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d2a834211251c3d7af0d6cf774eb661",
     "grade": true,
     "grade_id": "word_usage_analysis",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "From the data above we can see that the words frequently used by different major characters are so different. Bruce seems to have a strong interest in cars and planes. Jonathan mentioned a lot of names frequently so maybe he has many friends. Khloe uses the word 'fur' very often so she might be keen on luxury clothes. Both Kim and Koutney mentioned words related to violence and negative emotion quite a lot. Kris seems to be a nice person who loves chatting and shopping. Robert and Scott both talked a lot about sex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e25b394b4f8fa54b565cc97fc19df8d3",
     "grade": false,
     "grade_id": "cell-ed4cc4acf7ec52c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is the end of Assignment 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
